출처: 생활코딩
패키지 매니져 -> 앱스토어 같은 개념으로 보면 되겠다.

Python의 패키지 메니저
PyPI
Python Pakage Index. 파이썬의 패키지들을 정리해둔 사이트.

https://pypi.python.org/pypi

PIP
파이썬의 패키지 메니저.

Ruby의 패키지 메니저
RubyGems.org
루비의 패키지들을 정리해둔 사이트.

https://rubygems.org/

RubyGems
루비의 패키지 메니저.RubyGems.org에 등록된 패키지들을 쉽게 설치할 수 있도록 도와줍니다. 

ex)
pip install requests
--------------------
import requests
from bs4 import BeautifulSoup
r = requests.get('https://codingeverybody.github.io/scraping_sample/1.html')
soup = BeautifulSoup(r.text, 'html.parser')
print('Title : '+soup.title.string)
articles = soup.findAll('div', {'class' : 'em'})
print('Article : '+articles[0].text)
 
print("====================")
 
r = requests.get('https://codingeverybody.github.io/scraping_sample/2.html')
soup = BeautifulSoup(r.text, 'html.parser')
print('Title : '+soup.title.string)
articles = soup.findAll('div', {'class' : 'strong'})
print('Article : '+articles[0].text)

Study)
import requests
# When you are trying to crwaling codes using Python's bs4 and requests, you might get this error message caused by Encoding Korean etc.
# error -> UnicodeEncodeError: 'cp949' codec can't encode character '\xec' in position 1565: illegal multibyte sequence
# if you see this message, you can fix it with the 4 lines below to change encoding
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.detach(),encoding = 'utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(),encoding = 'utf-8')

from bs4 import BeautifulSoup
r = requests.get('http://yooshink.com/magazine_amang_travelessay.html')
print(r.text)
print("------------------------------------------------")
soup = BeautifulSoup(r.text, 'html.parser')
print(soup.title) # -> <title>Magazine - AMANG Korea</title>
print("------------------------------------------------")
print('Title is '+soup.title.string) # -> Title is Magazine - AMANG Korea
print("------------------------------------------------")
print(soup.title.name) # -> 'title'
print("------------------------------------------------")
print(soup.title.parent.name) # -> 'head'
print("------------------------------------------------")
print(soup.li) # 첫번재 li 태그만 출력 <li>2019.10. -<a href="./Amang Magazine/magazine file/amang_travel_2019_11.jpg" target="_blank">~~~~~</a></li>
print("------------------------------------------------")
print(soup.findAll("div",{"class":"magazine_page"})) # div태그에서 class명이 magazine_page인 녀석들만 출력
print("------------------------------------------------")
# soup.p['class']
# # u'title'
#
# soup.a
# # <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>

print(soup.find_all('a')) # 그 페이지에 있는 모든 a 태그 출력



